{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count Occurrence (Bag-of-Words):\n",
      "    and  document  first  is  one  second  the  third  this\n",
      "0    0         1      1   1    0       0    1      0     1\n",
      "1    0         2      0   1    0       1    1      0     1\n",
      "2    1         0      0   1    1       0    1      1     1\n",
      "3    0         1      1   1    0       0    1      0     1\n",
      "\n",
      "TF-IDF:\n",
      "         and  document     first        is       one    second       the  \\\n",
      "0  0.000000  0.469791  0.580286  0.384085  0.000000  0.000000  0.384085   \n",
      "1  0.000000  0.687624  0.000000  0.281089  0.000000  0.538648  0.281089   \n",
      "2  0.511849  0.000000  0.000000  0.267104  0.511849  0.000000  0.267104   \n",
      "3  0.000000  0.469791  0.580286  0.384085  0.000000  0.000000  0.384085   \n",
      "\n",
      "      third      this  \n",
      "0  0.000000  0.384085  \n",
      "1  0.000000  0.281089  \n",
      "2  0.511849  0.267104  \n",
      "3  0.000000  0.384085  \n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "import pandas as pd\n",
    "\n",
    "# Sample dataset\n",
    "data = [\n",
    "    'This is the first document.',\n",
    "    'This document is the second document.',\n",
    "    'And this is the third one.',\n",
    "    'Is this the first document?',\n",
    "]\n",
    "\n",
    "# Initialize CountVectorizer for bag-of-words (count occurrence)\n",
    "count_vectorizer = CountVectorizer()\n",
    "X_count = count_vectorizer.fit_transform(data)\n",
    "\n",
    "# Convert to DataFrame for better visualization\n",
    "count_df = pd.DataFrame(X_count.toarray(), columns=count_vectorizer.get_feature_names_out())\n",
    "print(\"Count Occurrence (Bag-of-Words):\\n\", count_df)\n",
    "\n",
    "# Initialize TfidfVectorizer for TF-IDF\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "X_tfidf = tfidf_vectorizer.fit_transform(data)\n",
    "\n",
    "# Convert to DataFrame for better visualization\n",
    "tfidf_df = pd.DataFrame(X_tfidf.toarray(), columns=tfidf_vectorizer.get_feature_names_out())\n",
    "print(\"\\nTF-IDF:\\n\", tfidf_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install gensim\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import ENGLISH_STOP_WORDS\n",
    "import pandas as pd\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding for 'document':\n",
      " [-5.3761393e-04  2.3459077e-04  5.1012170e-03  9.0115219e-03\n",
      " -9.3035055e-03 -7.1186870e-03  6.4577162e-03  8.9744031e-03\n",
      " -5.0161965e-03 -3.7644049e-03  7.3809391e-03 -1.5342169e-03\n",
      " -4.5370674e-03  6.5543531e-03 -4.8609949e-03 -1.8136933e-03\n",
      "  2.8776617e-03  9.8915887e-04 -8.2834894e-03 -9.4506554e-03\n",
      "  7.3119737e-03  5.0714435e-03  6.7562792e-03  7.6230383e-04\n",
      "  6.3530928e-03 -3.4065295e-03 -9.4848091e-04  5.7711215e-03\n",
      " -7.5222286e-03 -3.9373739e-03 -7.5092558e-03 -9.2885981e-04\n",
      "  9.5392875e-03 -7.3166536e-03 -2.3360765e-03 -1.9363161e-03\n",
      "  8.0779977e-03 -5.9297686e-03  4.5617318e-05 -4.7524953e-03\n",
      " -9.6023204e-03  5.0089518e-03 -8.7604597e-03 -4.3930719e-03\n",
      " -3.5214103e-05 -2.9548592e-04 -7.6621324e-03  9.6163880e-03\n",
      "  4.9832016e-03  9.2352722e-03 -8.1572160e-03  4.4980138e-03\n",
      " -4.1374546e-03  8.2675234e-04  8.4996261e-03 -4.4643688e-03\n",
      "  4.5164214e-03 -6.7876368e-03 -3.5471660e-03  9.3982853e-03\n",
      " -1.5784105e-03  3.2352045e-04 -4.1381051e-03 -7.6831253e-03\n",
      " -1.5093960e-03  2.4685122e-03 -8.8934030e-04  5.5310265e-03\n",
      " -2.7425813e-03  2.2589052e-03  5.4565049e-03  8.3474834e-03\n",
      " -1.4548642e-03 -9.2107793e-03  4.3709916e-03  5.6996895e-04\n",
      "  7.4426048e-03 -8.1429747e-04 -2.6364601e-03 -8.7528294e-03\n",
      " -8.5558271e-04  2.8258313e-03  5.4009468e-03  7.0547434e-03\n",
      " -5.7027498e-03  1.8572190e-03  6.0867225e-03 -4.7976980e-03\n",
      " -3.1054933e-03  6.7991368e-03  1.6290357e-03  1.9226066e-04\n",
      "  3.4747359e-03  2.1982045e-04  9.6214656e-03  5.0585950e-03\n",
      " -8.9198640e-03 -7.0399828e-03  9.0400706e-04  6.3935039e-03]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\shrey\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import Word2Vec\n",
    "from nltk.tokenize import word_tokenize\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "\n",
    "# Tokenize the data\n",
    "tokenized_data = [word_tokenize(text.lower()) for text in data]\n",
    "\n",
    "# Train Word2Vec model\n",
    "word2vec_model = Word2Vec(sentences=tokenized_data, vector_size=100, window=5, min_count=1, sg=0)\n",
    "\n",
    "# Get embeddings for words\n",
    "word_embeddings = {word: word2vec_model.wv[word] for word in word2vec_model.wv.index_to_key}\n",
    "\n",
    "# Example: Get embedding for a specific word\n",
    "word = 'document'\n",
    "if word in word_embeddings:\n",
    "    print(f\"Embedding for '{word}':\\n\", word_embeddings[word])\n",
    "else:\n",
    "    print(f\"'{word}' not in vocabulary.\")\n",
    "\n",
    "# Save or load the Word2Vec model\n",
    "# word2vec_model.save(\"word2vec.model\")\n",
    "# loaded_model = Word2Vec.load(\"word2vec.model\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
